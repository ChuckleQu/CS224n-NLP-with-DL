# CS224n-NLP-with-DL
Assignment for CS224n, winter, 2019.
## Lecture 01 - Introduction and Word Vectors
### How to represent the meaning of a word
For example glasses:
1. denotational semantics: refers to a specific pair of glasses
2. distributional meaning*: "glasses" defined by context
3. localist representation: a representation of a place stored, for instance, one-hot vectors
### Word2vec
The first lecture mainly discusses the Word2vec method. It will firstly define the likelihood function with the parameter theta and the hyperparameter m, where theta is the vector representation of the words.

After that, it constructs the cost function by taking the negative log-likelihood. For the conditional probability occurring in the cost function, it will be calculated through the softmax function.

After getting the cost function, we can use numerical methods to train the model, that is, to optimize the parameter theta. The lecturer introduces the stochastic gradient descent method, which is a kind of less-cost method than the batch gradient descent method.

## Lecture 02 - Word Vectors and Word Senses
### word2vect: Predict the surrounding words using word vectors
1. Skip-grams (SG): Predict context (outside) words given center words
2. Continuous Bag of Words (CBOW): Predict center word from context words
### Negative Sampling
The negative sampling can be added to optimize the skip-gram model. The idea of the negative sampling is to reduce the complexity when doing the optimization (or say, SGD). The general idea of how negative sampling works is that instead of taking the whole corpus to train the model (or to optimize the parameters), we can sample out only a few of the so-called "negative words" to train.

The "negative words" means the context words that are not in the window of the center word. About how to do the sampling, we use the unigram distribution with a power of 3/4, which results that the less frequent words will be sampled more often.

Actually, I think the expression "the less frequent words will be sampled more often" is not very precise. In fact, the more frequent words will still be sampled with greater probability than the rare ones. However, what this 3/4 power does is trying to reduce the difference of the chosen probability between the frequent words and the rare words, since you can have a view from the graph of y = x ^ (3/4), where it is closer to a uniform distribution (a horizontal line) than y = x.

Intuitively, I think the reason why the creators want to sample rare words more often is that if very frequent words like "the", "a", "an" are sampled too many times, it is some kind of meaningless to do the optimization, so we need the rare words as well. Maybe that's why they found out the power of 3/4 is appropriate in the end.

### Count Based vs. Direct Prediction
1. Count Based: GloVe
2. Direct Prediction: word2vec

## Lecture 03 - Neural Networks
### Classification Review and Notation
It's really exciting to see classification again since my FYP is talking about some classical classification models with my really really kind supervisor Gloria. Well, go back to our topic. When studying this part, I'm quite confused with "softmax" and "sigmoid", since the professor in the lecturer mentioned they are actually the same, but then said they are different in the number of neurons (if I remembered correctly), and sometimes the slides use softmax and sometimes use sigmoid (with sigma sign).

In fact, they are different when being the activation function in neural nets. Firstly, if we use the sigmoid function to do the multi-class classification, the addition of output probabilities does not necessarily be 1, maybe greater than 1, which means it allows overlap between each class. However, for softmax, the classes will be mutually exclusive, and the addition of the probabilities will be exact 1. 

Secondly, actually, I'm not sure whether I understand correctly. Well, taking binary classification for example, when using the sigmoid function, we will have one output neuron, but softmax will have two since sigmoid is kind of calculating the probability for one class, but softmax is actually calculating the probability for each class.

### Training with "cross entropy loss"
It's really exciting to see entropy as well since I've learned some so-called entropy in a project before. It's not hard to understand the "cross entropy loss". However, in the lecturer, there's no derivation, so it may look a little bit wired since it seems to come from nowhere, though in fact, it's quite reasonable. For people like me who have never heard of the "cross entropy loss", please do some search here.

### Neural Computation
So excited to get into the neural nets, though I still don't quite understand. Hope to learn more in Lecture 04.

## Lecture 04 - Backpropagation
Today's lecturer is not hard, since it's about partial derivatives and things like that. However, I think the example taken in the class isn't very straight-forward and coherent. I found a really good example with the real number participated in the computation. If you can read Chinese characters, may check this: https://blog.csdn.net/weixin_38347387/article/details/82936585. Really really good example.
